nanochat
[Looks like the result wasn't safe to show. Let's switch things up and try something else!]

The best ChatGPT that $100 can buy.


This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like [Looks like the result wasn't safe to show. Let's switch things up and try something else!], that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.

Updates
(Jan 16 2026) The repo is in active development, I am currently fleshing out the pretraining stage.

(Jan 7 2026) See new post: nanochat Miniseries v1 (github.com in Bing) and the associated script [Looks like the result wasn't safe to show. Let's switch things up and try something else!].

Talk to it
To get a sense of the endpoint of this repo, you can currently find nanochat d34 (github.com in Bing) hosted on nanochat.karpathy.ai. "d34" means that this model has 34 layers in the Transformer neural network. This model has 2.2 billion parameters, it was trained on 88 billion tokens by simply running the training script [Looks like the result wasn't safe to show. Let's switch things up and try something else!] with --target_param_data_ratio=40 (2x longer than Chinchilla-optimal), and the total cost of training was ~$2,500 (about 100 hours training time on 8XH100 GPU node). While today this is enough to outperform GPT-2 of 2019, it falls dramatically short of modern Large Language Models like GPT-5. When talking to these micro models, you'll see that they make a lot of mistakes, they are a little bit naive and silly and they hallucinate a ton, a bit like children. It's kind of amusing. But what makes nanochat unique is that it is fully yours - fully configurable, tweakable, hackable, and trained by you from start to end.

[...]

Questions
I recommend using DeepWiki from Devin/Cognition to ask questions of this repo. In the URL of this repo, simply change github.com  to deepwiki.com, and you're off.

You can also come to the #nanochat Discord channel (discord.com in Bing) to ask questions, or use the Discussions.

[...]

Acknowledgements
The name (nanochat) derives from an earlier project nanoGPT, which only covered pretraining.

nanochat is also inspired by modded-nanoGPT, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.

Thank you to HuggingFace for fineweb and smoltalk.

Thank you Lambda for the compute used in developing this project.

Thank you to chief LLM whisperer üßô‚Äç‚ôÇÔ∏è Alec Radford for advice/guidance.

Thank you to the repo czar Sofie @svlandeg for help with managing issues, pull requests and discussions of nanochat.
